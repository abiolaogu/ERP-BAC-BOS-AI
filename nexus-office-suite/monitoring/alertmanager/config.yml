# AlertManager Configuration for NEXUS Office Suite

global:
  resolve_timeout: 5m
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@nexus.example.com'
  smtp_auth_username: 'alerts@nexus.example.com'
  smtp_auth_password: '${SMTP_PASSWORD}'
  smtp_require_tls: true

  slack_api_url: '${SLACK_WEBHOOK_URL}'
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

# Templates
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route tree
route:
  receiver: 'default'
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h

  routes:
    # Critical alerts - immediate notification
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 5s
      repeat_interval: 1h
      routes:
        # Database critical alerts to DBA team
        - match:
            component: database
          receiver: 'dba-team'

        # Service down alerts to ops team
        - match_re:
            alertname: '.*Down'
          receiver: 'ops-team-pagerduty'

    # Warning alerts - less urgent
    - match:
        severity: warning
      receiver: 'warning-alerts'
      group_wait: 30s
      repeat_interval: 4h

    # Info alerts - lowest priority
    - match:
        severity: info
      receiver: 'info-alerts'
      repeat_interval: 24h

# Inhibition rules (suppress certain alerts when others are firing)
inhibit_rules:
  # Inhibit warning when critical is firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'instance']

  # Inhibit high error rate when service is down
  - source_match_re:
      alertname: '.*Down'
    target_match_re:
      alertname: 'High.*Rate'
    equal: ['service']

# Receivers
receivers:
  - name: 'default'
    email_configs:
      - to: 'ops-team@nexus.example.com'
        headers:
          Subject: '[NEXUS] {{ .GroupLabels.alertname }}'

  - name: 'critical-alerts'
    email_configs:
      - to: 'ops-team@nexus.example.com,management@nexus.example.com'
        headers:
          Subject: '[CRITICAL] {{ .GroupLabels.alertname }}'
        html: '{{ template "email.html" . }}'
    slack_configs:
      - channel: '#alerts-critical'
        title: 'Critical Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
    pagerduty_configs:
      - service_key: '${PAGERDUTY_SERVICE_KEY}'
        description: '{{ .GroupLabels.alertname }}: {{ .GroupLabels.instance }}'

  - name: 'warning-alerts'
    slack_configs:
      - channel: '#alerts-warning'
        title: 'Warning: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true
        color: 'warning'

  - name: 'info-alerts'
    slack_configs:
      - channel: '#alerts-info'
        title: 'Info: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true

  - name: 'dba-team'
    email_configs:
      - to: 'dba-team@nexus.example.com'
        headers:
          Subject: '[DATABASE CRITICAL] {{ .GroupLabels.alertname }}'
    pagerduty_configs:
      - service_key: '${PAGERDUTY_DBA_KEY}'

  - name: 'ops-team-pagerduty'
    pagerduty_configs:
      - service_key: '${PAGERDUTY_OPS_KEY}'
        description: '{{ .GroupLabels.alertname }}: {{ .GroupLabels.instance }}'
        severity: '{{ .CommonLabels.severity }}'
